Data Structures and Algorithms
    Big O Notation - 0(n)
-An algorithm is a precise set of step-by-step instructions designed to solve a specific problem or accomplish a particular task.
-We use big o to describe the performance of an algorithm.
This helps us determine if an algorithm is scalable or not.
That is, is this algorithm going to scale well if the input grows really large.
    Constant growth
Example 1 -  Big o of 1 - O(1)
printing the first element in an array
No matter the number of elements in an array(size of our input), to print the first item in an array, the time to execute the method is always constant
The run time complexity of printing the first element in an array is Big O of one, meaning it runs at constant time.
When talking about run time complexity, we just want to know how much of our algorithm slows down as the input grows.

    Linear growth
Example 2 - Big O of n - O(n)
Looping through an array and printing the elements of the array
-if we have a single item in an array, we will have one print operation
-if we have a million items, we will have a million print operations
Therefore the time complexity of this operation is denoted with the Big o of n - O(n)
Meaning the cost of this operation(time) increases linearly with the size of the input.

    Quadratic growth
Example 3 - Big O of n square - O(n ^ 2)
Nested loops
This is used to print all combinations of items in an array.
algorithms that run in O(n^2) runs slower as the input increases.
The run time complexity of this method is O(n2).
The first 3 examples have linear growth

    Logarithmic growth
The logarithmic curve slows down at some point while the linear curve grows at the same rate.
Meaning an algorithm that runs in logarithmic time is more efficient and scalable than an algorithm that runs in linear time.
Example 4 - Big O(log n) - Big O of log n

    Exponential growth
Big O(2^n) - Big O of 2 raise to power n
This is the opposite of the Logarithmic growth.
The curve grows faster and faster with increase input.
This is not scalable.

In an ideal word we want our algorithm to be super fast and take up small memory
but unfortunately it hardly ever happens.
Most of the time, we have to do a trade off between saving time and saving space.
    How much space an algorithm require(space complexity)
When we talk about space complexity, we only look at the additional space we should allocate relative to the size of the input.

    Data Structures
1)Arrays
This is used to store a list of items.
This items get stored sequentially in memory.
    Limitations of arrays
-arrays are static, meaning the size of an array cannot change.
so we have to know the size before hand to put elements in the array
if we guess, and the array size is bigger than the number of elements, then we waste memory
the run time complexity of adding elements in an array is O(n)
meaning the cost of copying into the array increases proportionally to the size of the array.
The time complexity for deleting the last element in the array is also O(n)
cos you have to go through all the elements and start changing their positions till you get to the last.
    Array class
-To see the content of an array, use the Array class
-Arrays in java are static, meaning the size cannot be changed
Exercise- Build a dynamic array

    Dynamic Array
Java has two implementations of dynamic arrays.
1)Vector: This class grows by 100% of its size when it get full.
All the methods of vector class are synchronized
2)ArrayList: This class grows by 50% of its size when it get full.
The methods in the ArrayList are not synchronized.
Both this classes are declared in the java.utils package.
-Synchronized: Only a single thread can access this method.
    How to declare an arrayList
ArrayList<wrapperType> arrayName = new ArrayList<>();
wrapperType - Integer, Byte, Boolean etc
    LinkedList
This is used to store a list of object in sequence.
Unlike arrays, a linklist can grow and shrink automatically
A linklist consist of a group of nodes in sequence.
    each nodes holds two pieces of values
-a value
-The address of the next node in the list
That is each nodes points to or references the next node.
That is why it is called a linkedlist cos all the nodes are linked together.
The first node is called the HEAD, and the last node the TAIL.
    Time complexity of linkedlist operations
When Thinking of the Time complexity, we consider the worse case scenario.
1)Looking up by value: O(n)
cos the worse case is if the value is at the end of the list, so we have to go through all the elements in the list.
2)Looking up by index: O(n)
Transverse the list till you find the element at the end.
3)Insertion at the end: O(1)
create a new node and have the last node(Tail) point to it.
4)Insertion at the Beginning: O(1)
create a new node and have the first node(Head) point to it.
Unlike arrays, we don't have to shift the elements, we just have to update the references.
5)Insertion at the Middle: O(n)
We first find the node in the middle and update the links
6)Deletion
From the Beginning O(1) operation
set the head to point to the second node
-from the end O(n) operation
transverse the list to the node before the last node and link it with the tail
-from the middle O(n) operation
    Build a linkedlist





